{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixnet旅遊熱門\n",
    "- 取得所有文章\n",
    "- 篩選出有日本字眼的 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 2):\n",
    "    chrome_options = Options() \n",
    "    chrome_options.add_argument('--headless')  #規避google bug\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(executable_path=\"chromedriver\", chrome_options=chrome_options)\n",
    "    driver.get('https://www.pixnet.net/blog/articles/category/29/hot/1'+str(i))\n",
    "    elem = driver.find_elements_by_css_selector('#articles h3')\n",
    "    artical_title = [i.text for i in elem]\n",
    "    elem = driver.find_elements_by_css_selector('#articles .author')\n",
    "    artical_author = [i.text for i in elem]\n",
    "    elem = driver.find_elements_by_css_selector('#articles p')\n",
    "    artical_content = [i.text for i in elem]\n",
    "    elem = driver.find_elements_by_css_selector('.meta')\n",
    "    artical_date = [i.text for i in elem]\n",
    "    elem = driver.find_elements_by_xpath('//*[(@id = \"articles\")]//p//a')\n",
    "    artical_url = [i.get_attribute('href') for i in elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( [artical_date, artical_author, artical_title, artical_content, artical_url], index=[\"artical_date\", \"artical_author\", \"artical_title\", \"artical_content\", \"artical_url\"]   )\n",
    "df=df.T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['artical_title'].str.contains('^.*?logo.*?$')]\n",
    "df[df['artical_title'].str.contains('^.*?2019.*?$')]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"artical_title\"].filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'pixnet_all_travel_article.csv', mode='a', encoding='utf_8_sig')   #存成csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 只抓tags, 日本旅遊的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while True:\n",
    " \n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    " \n",
    "    try:\n",
    " \n",
    "        # 定位页面底部的一个标题\n",
    " \n",
    "        driver.find_element_by_xpath('//*[@id=\"waterfall\"]/div[2]/div[33]/div/div/a')\n",
    " \n",
    "        # 如果没抛出异常就说明找到了底部标志，跳出循环\n",
    " \n",
    "        break\n",
    " \n",
    "    except NoSuchElementException as e:\n",
    " \n",
    "        # 抛出异常说明没找到底部标志，继续向下滑动\n",
    " \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome_options = Options() \n",
    "# chrome_options.add_argument('--headless')  #規避google bug\n",
    "# chrome_options.add_argument('--disable-gpu')\n",
    "#driver = webdriver.Chrome(executable_path=\"chromedriver\", chrome_options=chrome_options)\n",
    "\n",
    "def pixnet_cralwer_byTag(tag_name):\n",
    "    driver = webdriver.Chrome(executable_path=\"chromedriver\")\n",
    "    for i in tag_name:\n",
    "        driver.get('https://www.pixnet.net/tags/'+str(i)+'?filter=articles&sort=related')\n",
    "        # Get scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "        # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "            time.sleep(2)\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        \n",
    "        \n",
    "#         for i in range(50):\n",
    "#             if i:\n",
    "                #driver.execute_script('var action=document.documentElement.scrollTop=10000')\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            elem = driver.find_elements_by_xpath(\"//h2[@class='sc-1hu2j4t-2 fCYIqq']\")\n",
    "            article_title = [i.text for i in elem]\n",
    "            elem = driver.find_elements_by_xpath(\"//a[@class='sc-15yfh73-5 cIZsAM']\")\n",
    "            article_author = [i.text for i in elem]\n",
    "        #    elem = driver.find_elements_by_css_selector('#articles p')\n",
    "        #     article_content = [i.text for i in elem]\n",
    "            elem = driver.find_elements_by_xpath(\"//p[@class='sc-15yfh73-8 fQudZ']\")\n",
    "            article_date = [i.text for i in elem]\n",
    "            elem = driver.find_elements_by_xpath(\"//*/section[@class='sc-1ir7rbw-0 cIPhcA']/section/a\")\n",
    "        #    article_url = [i.text for i in elem]\n",
    "            article_url = [i.get_attribute('href') for i in elem]\n",
    "            elem = driver.find_elements_by_xpath(\"//div[@class='jxyoee-0 xGPZL']\")\n",
    "            article_tags = [i.text for i in elem]\n",
    "            print(\"scrolling..\")\n",
    "\n",
    "#            else:\n",
    "#                print(\"scroll finished\")\n",
    "    return article_date, article_author, article_title, article_url, article_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags = [\"日本旅遊\", \"日本旅行\", \"日本自助\", \"日本自由行\", \"東京\", \"大阪\", \"東京親子自由行\", \"#日本親子遊\", \"日本自駕\"]\n",
    "#tags = [\"日本資訊\", \"日本自駕\"]\n",
    "tags = [\"東京旅遊\", \"東京自由行\", \"京都住宿\", \"京都自由行\", \"日本賞櫻\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_date_list=[]\n",
    "article_author_list=[]\n",
    "article_title_list=[]\n",
    "article_url_list = []\n",
    "article_tags_list = []\n",
    "\n",
    "for i in tags:\n",
    "    article_date, article_author, article_title, article_url, article_tags = pixnet_cralwer_byTag([i])\n",
    "    article_date_list.extend(article_date)\n",
    "    article_author_list.extend(article_author)\n",
    "    article_title_list.extend(article_title)\n",
    "    article_url_list.extend(article_url)\n",
    "    article_tags_list.extend(article_tags)\n",
    "\n",
    "# df = pd.DataFrame([article_date_list, article_author_list, article_title_list, article_url_list, article_tags_list],\n",
    "#                        index=[\"article_date_list\", \"article_author_list\", \"article_title_list\", \"article_url_list\", \"article_tags_list\" ])\n",
    "# df=df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([article_date_list, article_author_list, article_title_list, article_url_list, article_tags_list],\n",
    "                       index=[\"article_date_list\", \"article_author_list\", \"article_title_list\", \"article_url_list\", \"article_tags_list\" ])\n",
    "df=df.T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'Pinex_20190604_bytags.csv', mode='a', encoding='utf_8_sig')   #存成csv檔"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "crawler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
